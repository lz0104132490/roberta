{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wj6eoKzotv5I"
   },
   "source": [
    "## Emotion Classification using Fine-tuned BERT model\n",
    "\n",
    "In this tutorial, I will show to fine-tune a language model (LM) for emotion classification with code adapted from this [tutorial](https://zablo.net/blog/post/custom-classifier-on-bert-model-guide-polemo2-sentiment-analysis/) by MARCIN ZAB≈ÅOCKI. I adapted his tutorial and modified the code to suit the emotion classification task using a different BERT model. Please refer to his tutorial for more detailed explanations for each code block. I really liked his tutorial because of the attention to detail and the use of high-level libraries to take care of certain parts of the model such as training and finding a good learning rate.\n",
    "\n",
    "Before you get started, make sure to enable `GPU` in the runtime and be sure to\n",
    "restart the runtime in this environment after installing the `pytorch-lr-finder` library.\n",
    "\n",
    "This tutorial is in a rough draft so if you find any issues with this tutorial or have any further questions reach out to me via [Twitter](https://twitter.com/omarsar0).\n",
    "\n",
    "Note that the notebook was created a little while back so if something break it's because the code is not compatible with the library changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G2tokZqttmTA"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install transformers tokenizers pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers==4.49.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0jZnNegGhZj"
   },
   "source": [
    "Note: you need to Restart runtime after running this code segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9ZKIIGvuW5m"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!git clone https://github.com/davidtvs/pytorch-lr-finder.git && cd pytorch-lr-finder && python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "qqRRWe4UuuIh",
    "outputId": "479b5e60-10b5-4d84-c8fc-291ec32feceb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from typing import List\n",
    "import torch.nn.functional as F\n",
    "from transformers import DistilBertTokenizer, AutoTokenizer, AutoModelWithLMHead, DistilBertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import logging\n",
    "import os\n",
    "from functools import lru_cache\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from argparse import Namespace\n",
    "from sklearn.metrics import classification_report\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_whSBDujRiga"
   },
   "source": [
    "## Load the Pretrained Language Model\n",
    "We are first going to look at pretrained language model provided by HuggingFace models. We will use a variant of BERT, called DistilRoBERTa base. The `base` model has less parameters than the `larger` model.\n",
    "\n",
    "[RoBERTa](https://arxiv.org/abs/1907.11692) is a variant of of BERT which \"*modifies key hyperparameters, removing the next-sentence pretraining objective and training with much larger mini-batches and learning rates*\".\n",
    "\n",
    "Knowledge distillation help to train smaller LMs with similar performance and potential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvHNcMckSR4M"
   },
   "source": [
    "First, let's load the tokenizer for this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "bfa062ba95014af8993654adebeed30b",
      "494133cceef14cae8fa7efa5d18bfb45",
      "522e5a6c954c4297b21f8e0cea44a171",
      "269ff42456ee47bba72aca0923542c29",
      "511c63b5846447f1a98f99efa7b414ef",
      "9f2624b1ec3a4de3aeac12b403131745",
      "55662051a2f845c2a9de8fc27eea8485",
      "b93fa5f05abe46f8a15cd4b93f743243",
      "6b01657a3a0f47569757d62cff6970bd",
      "8fcfdc4bfbf74c73a38aa13175269565",
      "352725603b554637bceb08047c134523",
      "7b89a5097be146cb94844ff9af75bb77",
      "dd64efe54437454c9d919530cceadff7",
      "ccdae4645e744fd1a083d857d0e56a0a",
      "9404f7978dba4adfb4b757b8446bfae5",
      "43fdffaac68940beb54c21e0f5d21135",
      "0e3236fcfc07425ab83f1b158e3c0f00",
      "cb4074b47b464adb863bf9d8048ff10a",
      "bc402b2b5ac64749abee5ef047a78022",
      "b5cc3132707b4ee5885bc3bab6b384f4",
      "4f896b64ee4c4102aa5826f3927ed297",
      "eb8e48d71030482286bba58939a3ba25",
      "2ab5305315964703bf834e87f1881aaf",
      "aa7dcbcebf7e450da1d472da998b93ab",
      "728bb27db36d49319bcd129245ce2ade",
      "f8c4c65ab8eb4602b9cc67a18f17a80f",
      "52f517a67ec549d8bb3d952bc5c8471e",
      "a065514ff5ab45418085d2b5450c81ea",
      "09efbdba1ef1407aab615418a44a6571",
      "ae1118ec19ec4fb095c1e5616c366ee2",
      "a67576e0883e45de9f7c6e25d32601e4",
      "f80c7217e70b408684909176198e8720",
      "e894699ebf924a978689b1754e28449e",
      "93f62d912f43409ab8c9bd245b137bde",
      "0df8766d48974f799cf4a253ebfd8206",
      "a6f26948e4fe449b99e58a584be39492",
      "d1519847727e42a6b6a46fd1b425068c",
      "843b224c4f454a2695bbc99869987d80",
      "3f5081a276b04d38aca64760de0a078e",
      "4f51720e43824ae59e343090b4718f15",
      "697378036251425d965f8e52b36cfa60",
      "e940a875278a4ce297c7fe35be8014f8",
      "7306f377658349a5a36123251c68be41",
      "0a8b8f25fda04d358c8eb4575b5998fd"
     ]
    },
    "id": "BPbTd5lmuzQn",
    "outputId": "68c9cb51-f420-45a7-e500-235fd96c2038"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KAbKMqJSWRo"
   },
   "source": [
    "Now let's load the actual model with the LM head that takes care of the prediciton for the LM. When fine-tuning we don't use the head and instead use the base model. The code below shows how to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "950b4d43541b44cc959d62b45c6d14e3",
      "6a85a6adc199436f89b9038868ae0f48",
      "2fd1ea2ed94447f1b6cc11e407aca6f2",
      "2cd0df845e9f414e8c3a9c5d7dd9eac1",
      "8d421622ad4146cfb83237de735d3e34",
      "d1aea3f85fbb40b1ae318bcc5044b2f1",
      "0f3f0061d0e54bbbb274c5302463e67f",
      "c01bbd73be554b199bc3cfac4fd25952",
      "391e7c73480b41209fd65fbacebe9e76",
      "4d130af1b2a742869aabb3bdf7402331",
      "59d8e41de3c8447ab01d1b525709f6ae"
     ]
    },
    "id": "PCXYlMydzQlP",
    "outputId": "872c8d52-a9d3-4848-b369-6faedd5165c0"
   },
   "outputs": [],
   "source": [
    "model = AutoModelWithLMHead.from_pretrained(\"distilroberta-base\")\n",
    "base_model = model.base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2_8S8BXSpNa"
   },
   "source": [
    "Let's now try out the tokenizer first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5fidSmH-zrY_",
    "outputId": "b90e076e-5b23-44f2-ec35-aa3a9849fac4"
   },
   "outputs": [],
   "source": [
    "text = \"Elvis is the king of rock!\"\n",
    "enc = tokenizer.encode_plus(text)\n",
    "enc.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m8F8yQCDTDQi",
    "outputId": "30c4c9d9-d559-44a8-af7e-3482324a39b4"
   },
   "outputs": [],
   "source": [
    "print(enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3wSCLKW0ndh"
   },
   "source": [
    "`input_ids` are the numerical encoding of the tokens in the vocabulary. `attention_mask` is an addition option used when batching sequences together and you want to tell the model which tokens should be attented to ([read more](https://huggingface.co/transformers/glossary.html#attention-mask)). The attention mask information helps when dealing with variance in the size of sequences and we need a way to tell the model that we don't want to attend to the padded indices of the sequence.\n",
    "\n",
    "We are only using `input_ids` and `attention_mask`\n",
    "\n",
    "We need to also unsqueeze to simulate batch processing\n",
    "\n",
    "Using DistilBertForSequenceClassification: https://huggingface.co/transformers/model_doc/distilbert.html#distilbertforsequenceclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mxsts4uT0PgA",
    "outputId": "b4f42ac5-7577-464d-a346-90eec73b8b28"
   },
   "outputs": [],
   "source": [
    "out = base_model(torch.tensor(enc[\"input_ids\"]).unsqueeze(0), torch.tensor(enc[\"attention_mask\"]).unsqueeze(0))\n",
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZiCO-n_1AHIf",
    "outputId": "4ea312ee-7ba1-458d-dbc4-31e9d05596bd"
   },
   "outputs": [],
   "source": [
    "## size of representation of one of the tokens\n",
    "out[0][:,0,:].shape, out[0].shape, out[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srwIb9nr4g4t"
   },
   "source": [
    "`torch.Size([1, 768])` represents batch_size, number of tokens in input text (lenght of tokenized text), model's output hidden size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iAsg0H6g53Bf",
    "outputId": "9cac2261-c90e-4bbc-8db2-2bbaa458035f"
   },
   "outputs": [],
   "source": [
    "t = \"Elvis is the king of rock\"\n",
    "enc = tokenizer.encode_plus(t)\n",
    "token_representations = base_model(torch.tensor(enc[\"input_ids\"]).unsqueeze(0))[0][0]\n",
    "print(enc[\"input_ids\"])\n",
    "print(tokenizer.decode(enc[\"input_ids\"]))\n",
    "print(f\"Length: {len(enc['input_ids'])}\")\n",
    "print(token_representations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RFifOoY7Hsc"
   },
   "source": [
    "## Building Custom Classification head on top of LM base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSUMm4Oq7nvR"
   },
   "source": [
    "Use Mish activiation function as in the one proposed in the original tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCEDXLxq628O"
   },
   "outputs": [],
   "source": [
    "# from https://github.com/digantamisra98/Mish/blob/b5f006660ac0b4c46e2c6958ad0301d7f9c59651/Mish/Torch/mish.py\n",
    "@torch.jit.script\n",
    "def mish(input):\n",
    "    return input * torch.tanh(F.softplus(input))\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return mish(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6Ln6KWm74ku"
   },
   "source": [
    "The model we will use to do the fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9VDRSRsc71H2"
   },
   "outputs": [],
   "source": [
    "class EmoModel(nn.Module):\n",
    "    def __init__(self, base_model, n_classes, base_model_output_size=768, dropout=0.05):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(base_model_output_size, base_model_output_size),\n",
    "            Mish(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(base_model_output_size, n_classes)\n",
    "        )\n",
    "\n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.weight.data.normal_(mean=0.0, std=0.02)\n",
    "                if layer.bias is not None:\n",
    "                    layer.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input_, *args):\n",
    "        X, attention_mask = input_\n",
    "        hidden_states = self.base_model(X, attention_mask=attention_mask)\n",
    "\n",
    "        # maybe do some pooling / RNNs... go crazy here!\n",
    "\n",
    "        # use the <s> representation\n",
    "        return self.classifier(hidden_states[0][:, 0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjgME-3O8Yfo"
   },
   "source": [
    "### Pretest the model with dummy text\n",
    "We want to ensure that the model is returing the right information back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y6H9eF8A8XeV",
    "outputId": "4cc0eccb-40bb-420d-f679-7fa3b548b9a6"
   },
   "outputs": [],
   "source": [
    "classifier = EmoModel(AutoModelWithLMHead.from_pretrained(\"distilroberta-base\").base_model, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-sjfHJ_L9iNH"
   },
   "outputs": [],
   "source": [
    "X = torch.tensor(enc[\"input_ids\"]).unsqueeze(0).to('cpu')\n",
    "attn = torch.tensor(enc[\"attention_mask\"]).unsqueeze(0).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o6QhCuEC-y2z",
    "outputId": "2dd22943-cb2c-4235-faae-a9024cbf69ec"
   },
   "outputs": [],
   "source": [
    "classifier((X, attn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-N7WSY7Cb7v"
   },
   "source": [
    "## Prepare your dataset for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDWkjaLV-5tj"
   },
   "outputs": [],
   "source": [
    "!mkdir -p tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wMMm5Ye1Db-m",
    "outputId": "0babb6a0-f763-4684-82f9-55819619a807"
   },
   "outputs": [],
   "source": [
    "## load pretrained tokenizer information\n",
    "tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhTEgIaLEDRo"
   },
   "source": [
    "Implement CollateFN using fast tokenizers.\n",
    "This function basically takes care of proper tokenization and batches of sequences. This way you don't need to create your batches manually. Find out more about Tokenizers [here](https://github.com/huggingface/tokenizers/tree/master/bindings/python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3SCLBZsMDn4s"
   },
   "outputs": [],
   "source": [
    "class TokenizersCollateFn:\n",
    "    def __init__(self, max_tokens=512):\n",
    "\n",
    "        ## RoBERTa uses BPE tokenizer similar to GPT\n",
    "        t = ByteLevelBPETokenizer(\n",
    "            \"tokenizer/vocab.json\",\n",
    "            \"tokenizer/merges.txt\"\n",
    "        )\n",
    "        t._tokenizer.post_processor = BertProcessing(\n",
    "            (\"</s>\", t.token_to_id(\"</s>\")),\n",
    "            (\"<s>\", t.token_to_id(\"<s>\")),\n",
    "        )\n",
    "        t.enable_truncation(max_tokens)\n",
    "        t.enable_padding(length=max_tokens, pad_id=t.token_to_id(\"<pad>\"))\n",
    "        self.tokenizer = t\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        encoded = self.tokenizer.encode_batch([x[0] for x in batch])\n",
    "        sequences_padded = torch.tensor([enc.ids for enc in encoded])\n",
    "        attention_masks_padded = torch.tensor([enc.attention_mask for enc in encoded])\n",
    "        labels = torch.tensor([x[1] for x in batch])\n",
    "\n",
    "        return (sequences_padded, attention_masks_padded), labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hu70Ng0Eqls"
   },
   "source": [
    "## Getting the Data and Preview it\n",
    "Below we are going to load the data and show you how to create the splits. However, we don't need to split the data manually becuase I have already created the splits and stored those files seperately which you can quickly download below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_03fxufWX_G"
   },
   "outputs": [],
   "source": [
    "## export the datasets as txt files\n",
    "## EXERCISE: Change this to an address\n",
    "\n",
    "train_path = \"train.txt\"\n",
    "test_path = \"test.txt\"\n",
    "val_path = \"val.txt\"\n",
    "\n",
    "## emotion labels\n",
    "label2int = {\n",
    "  \"sadness\": 0,\n",
    "  \"joy\": 1,\n",
    "  \"love\": 2,\n",
    "  \"anger\": 3,\n",
    "  \"fear\": 4,\n",
    "  \"surprise\": 5\n",
    "}\n",
    "\n",
    "emotions = [ \"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJm31gKShQus"
   },
   "source": [
    "## Split the data and store into individual text files\n",
    "\n",
    "If you are using your own dataset and want to split it for training, you can uncomment the code below. Otherwise, just skip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ooNxSnPiztL"
   },
   "outputs": [],
   "source": [
    "## uncomment the code below to generate the text files for your train, val, and test datasets.\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Âä†ËΩΩÊÉÖÊÑüÊï∞ÊçÆÈõÜ\n",
    "ds = load_dataset(\"dair-ai/emotion\")\n",
    "\n",
    "# ÊèêÂèñÊï∞ÊçÆ\n",
    "train_data = ds[\"train\"]\n",
    "validation_data = ds[\"validation\"]\n",
    "test_data = ds[\"test\"]\n",
    "\n",
    "# ËΩ¨Êç¢‰∏∫ DataFrame\n",
    "input_train = train_data[\"text\"]\n",
    "target_train = train_data[\"label\"]\n",
    "\n",
    "input_val = validation_data[\"text\"]\n",
    "target_val = validation_data[\"label\"]\n",
    "\n",
    "input_test = test_data[\"text\"]\n",
    "target_test = test_data[\"label\"]\n",
    "\n",
    "# ÊûÑÂª∫ DataFrame\n",
    "train_dataset = pd.DataFrame(data={\"text\": input_train, \"class\": target_train})\n",
    "val_dataset = pd.DataFrame(data={\"text\": input_val, \"class\": target_val})\n",
    "test_dataset = pd.DataFrame(data={\"text\": input_test, \"class\": target_test})\n",
    "\n",
    "# # ÂèØÈÄâÔºöÊâìÂåÖ‰∏∫ dict\n",
    "# final_dataset = {\"train\": train_dataset, \"val\": val_dataset , \"test\": test_dataset }\n",
    "\n",
    "# ‰øùÂ≠ò‰∏∫ CSVÔºåÊ≥®ÊÑè‰Ω†ÁöÑË∑ØÂæÑÂèòÈáè train_path„ÄÅval_path„ÄÅtest_path Ë¶ÅÈ¢ÑÂÖàÂÆö‰πâÂ•Ω\n",
    "train_dataset.to_csv(train_path, sep=\";\", header=False, index=False)\n",
    "val_dataset.to_csv(val_path, sep=\";\", header=False, index=False)\n",
    "test_dataset.to_csv(test_path, sep=\";\", header=False, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAD1J6c0dLp8"
   },
   "source": [
    "## Create the Dataset object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOOI69vwIYcN"
   },
   "source": [
    "Create the Dataset object that will be used to load the different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ktr6xeMuISin"
   },
   "outputs": [],
   "source": [
    "class EmoDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        super().__init__()\n",
    "        self.data_column = \"text\"\n",
    "        self.class_column = \"class\"\n",
    "        self.data = pd.read_csv(path, sep=\";\", header=None, names=[self.data_column, self.class_column],\n",
    "                               engine=\"python\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data.loc[idx, self.data_column], int(self.data.loc[idx, self.class_column])\n",
    "\n",
    "    def __len__(self):  \n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EYQRq3qJH7n"
   },
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uGWw4wGEJGhJ",
    "outputId": "e0983e49-770d-495f-a4d4-50d499ff4011"
   },
   "outputs": [],
   "source": [
    "ds = EmoDataset(train_path)\n",
    "ds[19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0h6tTn9hd6v8"
   },
   "source": [
    "## Training with PyTorchLightning\n",
    "\n",
    "[PyTorchLightning](https://www.pytorchlightning.ai/) is a library that abstracts the complexity of training neural networks with PyTorch. It is built on top of PyTorch and simplifies training.\n",
    "\n",
    "![](https://pytorch-lightning.readthedocs.io/en/latest/_images/pt_to_pl.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJHhNRcZK7sV"
   },
   "outputs": [],
   "source": [
    "## Methods required by PyTorchLightning\n",
    "\n",
    "class TrainingModule(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.model = EmoModel(AutoModelWithLMHead.from_pretrained(\"distilroberta-base\").base_model, len(emotions))\n",
    "        self.loss = nn.CrossEntropyLoss() ## combines LogSoftmax() and NLLLoss()\n",
    "        #self.hparams = hparams\n",
    "        self.hparams.update(vars(hparams))\n",
    "\n",
    "    def step(self, batch, step_name=\"train\"):\n",
    "        X, y = batch\n",
    "        loss = self.loss(self.forward(X), y)\n",
    "        loss_key = f\"{step_name}_loss\"\n",
    "        tensorboard_logs = {loss_key: loss}\n",
    "\n",
    "        return { (\"loss\" if step_name == \"train\" else loss_key): loss, 'log': tensorboard_logs,\n",
    "               \"progress_bar\": {loss_key: loss}}\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        return self.model(X, *args)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"val\")\n",
    "\n",
    "    def validation_end(self, outputs: List[dict]):\n",
    "        loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        return {\"val_loss\": loss}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"test\")\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.create_data_loader(self.hparams.train_path, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.create_data_loader(self.hparams.val_path)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.create_data_loader(self.hparams.test_path)\n",
    "\n",
    "    def create_data_loader(self, ds_path: str, shuffle=False):\n",
    "        return DataLoader(\n",
    "                    EmoDataset(ds_path),\n",
    "                    batch_size=self.hparams.batch_size,\n",
    "                    shuffle=shuffle,\n",
    "                    collate_fn=TokenizersCollateFn()\n",
    "        )\n",
    "\n",
    "    @lru_cache()\n",
    "    def total_steps(self):\n",
    "        return len(self.train_dataloader()) // self.hparams.accumulate_grad_batches * self.hparams.epochs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        ## use AdamW optimizer -- faster approach to training NNs\n",
    "        ## read: https://www.fast.ai/2018/07/02/adam-weight-decay/\n",
    "        optimizer = AdamW(self.model.parameters(), lr=self.hparams.lr)\n",
    "        lr_scheduler = get_linear_schedule_with_warmup(\n",
    "                    optimizer,\n",
    "                    num_warmup_steps=self.hparams.warmup_steps,\n",
    "                    num_training_steps=self.total_steps(),\n",
    "        )\n",
    "        return [optimizer], [{\"scheduler\": lr_scheduler, \"interval\": \"step\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGc7Vw1moHxr"
   },
   "source": [
    "## Finding Learning rate for the model\n",
    "\n",
    "The code below aims to obtain valuable information about the optimal learning rate during a pretraining run. Determine boundary and increase the leanring rate linearly or exponentially.\n",
    "\n",
    "More: https://github.com/davidtvs/pytorch-lr-finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474,
     "referenced_widgets": [
      "69c22be4a46740149cf5ea6823a5eed9",
      "7c615a5ac40b43ad80f49e5b520c7cd5",
      "b4038fd3724d429fb0026753c504c40d",
      "c8b4362c1387438d8bff0a98838c62f6",
      "3c8b0bd72bb34489ba4f7336b04127be",
      "4599afc67ec241fb879ac419d92cb01d",
      "abb8e33775974bdb8718d09b1ba6875b",
      "66f1cd46a86e44558cf2657469a63c62",
      "e0d5d58dd5584898a7fe092a9fb371fb",
      "fd994e14a67a4adf80a480b6ee20afa8",
      "2e849223bca94e42a426180c54ecd8fb"
     ]
    },
    "id": "xL4lNPDFoFyU",
    "outputId": "aa34657a-62da-4718-8495-768b30c9849e"
   },
   "outputs": [],
   "source": [
    "lr=0.1 ## uper bound LR\n",
    "from torch_lr_finder import LRFinder\n",
    "hparams_tmp = Namespace(\n",
    "    train_path=train_path,\n",
    "    val_path=val_path,\n",
    "    test_path=test_path,\n",
    "    batch_size=16,\n",
    "    warmup_steps=100,\n",
    "    epochs=1,\n",
    "    lr=lr,\n",
    "    accumulate_grad_batches=1,\n",
    ")\n",
    "module = TrainingModule(hparams_tmp)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(module.parameters(), lr=5e-7) ## lower bound LR\n",
    "lr_finder = LRFinder(module, optimizer, criterion, device=\"cuda\")\n",
    "lr_finder.range_test(module.train_dataloader(), end_lr=100, num_iter=100, accumulation_steps=hparams_tmp.accumulate_grad_batches)\n",
    "lr_finder.plot()\n",
    "lr_finder.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YdqP56M1oXav",
    "outputId": "f6c0b138-6248-4543-b987-6d7430c015f1"
   },
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "id": "vMab6vu0Bow0",
    "outputId": "d3da4b83-d2ea-4f28-988a-61e796ae210b"
   },
   "outputs": [],
   "source": [
    "lr_finder.plot(show_lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhHutCseBxjJ"
   },
   "source": [
    "## Training the Emotion Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q3FiLr3LBrjs",
    "outputId": "cda9d5ff-144a-4bcb-a404-b313ddf102de"
   },
   "outputs": [],
   "source": [
    "hparams = Namespace(\n",
    "    train_path=train_path,\n",
    "    val_path=val_path,\n",
    "    test_path=test_path,\n",
    "    batch_size=32,\n",
    "    warmup_steps=100,\n",
    "    epochs=5,\n",
    "    lr=lr,\n",
    "    accumulate_grad_batches=1\n",
    ")\n",
    "module = TrainingModule(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8Jv_U25B37g"
   },
   "outputs": [],
   "source": [
    "## garbage collection\n",
    "import gc; gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867,
     "referenced_widgets": [
      "163afdc0b9264042b077d18e0eb52a6b",
      "1cb6a608fb424ebba37963363dd0bfbe",
      "8da4e895f98845e08267b25d32eeef50",
      "5193bb81089f4b0f8e4edd5b3f5217f3",
      "b0f51d2ab93c4eba9a43a5a05935600c",
      "8724d5cfe6a6463f83e9ad5f5954607b",
      "179400f1591c4b42980ad2a0b3fc785e",
      "2a7a08f6d7484ff1a3174d68ca167b8e",
      "096cd19c21354a7dac1c45daa1af460d",
      "a157658902f34eb3a7c63cbfcb3e53c7",
      "24b055e798604635abe3c346845cdacc",
      "7ab54ba7ccc9484a8c8b2605d35a6a9c",
      "55690e54a4ce492bbd234fd380be63ee",
      "85007e2fd3204764b63fbf2277c1005d",
      "84201a6f02fb405ea11b7716ab204a04",
      "7c368c0d395f492dbf31beadd460e281",
      "f4a53c58c5c64fd094d30aa26cd4d1d9",
      "f73a02a5670a4d66a3dc8e386a2f56e5",
      "db921fe611024aa9adcc3caa4dc89a39",
      "91dae13977c347079c8c2421acae1e67",
      "af78532809f14e63856e426d601e807e",
      "4b9f99f4c5ad44fa8f6c841bc00d8868"
     ]
    },
    "id": "oRnl4HXvB5-T",
    "outputId": "b3c07d45-af92-4226-f68a-bb97cc1c215e"
   },
   "outputs": [],
   "source": [
    "## train roughly for about 10-15 minutes with GPU enabled.\n",
    "trainer = pl.Trainer(devices=1, accelerator=\"gpu\", max_epochs=hparams.epochs,\n",
    "                     accumulate_grad_batches=hparams.accumulate_grad_batches)\n",
    "\n",
    "trainer.fit(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U0_Z_4Pkl3fc",
    "outputId": "a45a771a-1566-4ef7-f935-52633d499c4b"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
